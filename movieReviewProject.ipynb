{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Modules !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data - Data !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input-ting the X and Y datasets !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"/home/khushvirmani/Desktop/wholeDesktop/movieReviewProject/imdbTrainDataX.txt\", 'r')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "h = open(\"/home/khushvirmani/Desktop/wholeDesktop/movieReviewProject/imdbTrainDataY.txt\", 'r')\n",
    "ratings = h.read()\n",
    "h.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "rated = ratings.split(\"\\n\")\n",
    "ratingsList = rated[:-1]\n",
    "print(len(ratingsList))\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the data and then cleaning the data by StopWord removal and stemming !\n",
    "def wordClean(string):\n",
    "    # Tokenize\n",
    "    tokenizer = RegexpTokenizer(\"[a-zA-Z']+\")\n",
    "    textPh1 = tokenizer.tokenize(string.lower())\n",
    "    \n",
    "    # Stopword removal\n",
    "    stpwrd = stopwords.words('english')\n",
    "    textPh2 = [ix for ix in textPh1 if ix not in stpwrd]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = SnowballStemmer('english') \n",
    "    textPh3 = [stemmer.stem(ix) for ix in textPh2]\n",
    "    \n",
    "    return textPh3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean Training data !\n",
    "cleanReviews  = [ ]\n",
    "for ix in lines:\n",
    "    wd = wordClean(ix)\n",
    "    cleanReviews.append(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training labels !\n",
    "ratingsTrain = []\n",
    "for jx in ratingsList:\n",
    "    ratingsTrain.append(int(jx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to check the data !!\n",
    "\n",
    "# for ix in range(4):\n",
    "#     print(cleanReviews[ix])\n",
    "#     print(ratingsTrain[ix])\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "# print(len(cleanReviews))\n",
    "# print(len(ratingsTrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating workable Vocabulary !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of each review label .\n",
    "\n",
    "priorCount = { }\n",
    "for ix in ratingsTrain:\n",
    "    if priorCount.get(ix) == None:\n",
    "        priorCount[ix] = 1\n",
    "    else:\n",
    "        priorCount[ix] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior Probability of each review label .\n",
    "\n",
    "priorProb = { }\n",
    "for ix in priorCount:\n",
    "    priorProb[ix] = priorCount[ix]/float(len(ratingsTrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total frequency of each word w.r.t. each class !\n",
    "\n",
    "wordFreqPerCLass = { }\n",
    "for ix in range(len(cleanReviews)):\n",
    "    for jx in cleanReviews[ix]:\n",
    "        if wordFreqPerCLass.get(jx) == None:\n",
    "            wordFreqPerCLass[jx] = { }\n",
    "        else:    \n",
    "            if wordFreqPerCLass[jx].get(ratingsTrain[ix]) == None:\n",
    "                wordFreqPerCLass[jx][ratingsTrain[ix]] = 1\n",
    "            else:\n",
    "                wordFreqPerCLass[jx][ratingsTrain[ix]] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global labels, as review categories 6,7 do not exist !\n",
    "\n",
    "labelArgmax = {0:1,1:2,2:3,3:4,4:7,5:8,6:9,7:10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the total frequncy of each word in all reviews .\n",
    "## NOT REQUIRED !!\n",
    "\n",
    "# wordsFreq = { }\n",
    "# for ix in cleanReviews:\n",
    "#     for jx in ix:\n",
    "#         if wordsFreq.get(jx) == None:\n",
    "#             wordsFreq[jx] = 1\n",
    "#         else:\n",
    "#             wordsFreq[jx]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total words in each class !\n",
    "## NOT Required !\n",
    "\n",
    "# wordsInClasses = {1:0,2:0,3:0,4:0,7:0,8:0,9:0,10:0}\n",
    "# for ix in range(len(cleanReviews)):\n",
    "#     wordsInClasses[ratingsTrain[ix]]+= len(cleanReviews[ix])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf scores !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tfIdf(tf_word , total_docs, docs_present):\n",
    "#     return tf * math.log2((total_docs+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MAIN code !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('/home/khushvirmani/Desktop/wholeDesktop/movieReviewProject/imdbtestDataX.txt')\n",
    "testReviews = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = open(\"/home/khushvirmani/Desktop/wholeDesktop/movieReviewProject/imdbtestDataY.txt\")\n",
    "testY = g.read()\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "testReviews = testReviews.split(\"\\n\")\n",
    "cleanTestReviews  = [ ]\n",
    "for ix in testReviews:\n",
    "    wdTest = wordClean(ix)\n",
    "    cleanTestReviews.append(wdTest)\n",
    "cleanTestReviews = cleanTestReviews[:25000]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLabels = testY.split(\"\\n\")\n",
    "testLabels = testLabels[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT to check the test data !\n",
    "\n",
    "# for ix in range(4):\n",
    "#     print(cleanTestReviews[ix])\n",
    "#     print(testLabels[ix])\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedY = []\n",
    "for ix in cleanTestReviews[:25000]:\n",
    "    tempLst = []\n",
    "    for kx in labelArgmax.values():\n",
    "         # for hloding respective probabilities of classes !\n",
    "        posteriorProbab = priorProb[kx]\n",
    "        for jx in ix:\n",
    "            if jx in wordFreqPerCLass.keys():\n",
    "                if kx in wordFreqPerCLass[jx].keys():\n",
    "                    posteriorProbab *= float(wordFreqPerCLass[jx][kx])/priorCount[kx]\n",
    "                else:\n",
    "                    posteriorProbab *= float(1/(priorCount[kx]))\n",
    "            else:\n",
    "                posteriorProbab *= float(1/(priorCount[kx]))\n",
    "        tempLst.append(posteriorProbab)\n",
    "#     print(tempLst)\n",
    "    predictedY.append(labelArgmax[np.argmax(tempLst)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4370\n"
     ]
    }
   ],
   "source": [
    "cx = 0\n",
    "for zx in range(len(predictedY)):\n",
    "    if int(testLabels[zx]) == predictedY[zx]:\n",
    "        cx+=1\n",
    "print(cx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedTrainY = []\n",
    "for ix in cleanReviews[:25000]:\n",
    "    tempLst = []\n",
    "    for kx in labelArgmax.values():\n",
    "         # for hloding respective probabilities of classes !\n",
    "        posteriorProbab = priorProb[kx]\n",
    "        for jx in ix:\n",
    "            if jx in wordFreqPerCLass.keys():\n",
    "                if kx in wordFreqPerCLass[jx].keys():\n",
    "                    posteriorProbab *= float(wordFreqPerCLass[jx][kx])/priorCount[kx]\n",
    "                else:\n",
    "                    posteriorProbab *= float(1/(priorCount[kx]))\n",
    "            else:\n",
    "                posteriorProbab *= float(1/(priorCount[kx]))\n",
    "        tempLst.append(posteriorProbab)\n",
    "#     print(tempLst)\n",
    "    predictedTrainY.append(labelArgmax[np.argmax(tempLst)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8577\n"
     ]
    }
   ],
   "source": [
    "cz = 0\n",
    "for zx in range(len(predictedTrainY)):\n",
    "    if int(ratingsTrain[zx]) == predictedTrainY[zx]:\n",
    "        cz+=1\n",
    "print(cz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train data:  34.308\n",
      "Accuracy on train data:  17.48\n"
     ]
    }
   ],
   "source": [
    "# Accuracy on train data !\n",
    "\n",
    "\n",
    "print(\"Accuracy on train data: \",(float(cz)/ 25000)*100)\n",
    "\n",
    "# Accuracy on test data !\n",
    "\n",
    "print(\"Accuracy on train data: \",(float(cx)/25000) *100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
